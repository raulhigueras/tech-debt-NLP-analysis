# tech-debt-NLP-analysis

Natural Language Processing analysis of developers' written messages in the Technical Debt Database to predict issues' difficulty.

## Table of Content

1. [Project Status](#project-status)
2. [Instructions of Use](#instructions-of-use)
3. [Project Organization](#project-organization)


## Project Status

- [x] Problem Understanding
- [x] Data Exploration 
- [x] Data Preprocessing
- [x] Modeling
- [x] Evaluation
- [x] Deploy

All finished! ðŸ˜Š


## Instructions of use

### Setup
First of all, run the following commands to install all the requirements and download the dataset.
```{bash}
make requirements
make data
```

### Data Exploration
The data exploration part is all contained in the notebook `001-exploration.ipynb`. To see the whole analysis, just run the notebook from the start of the document.

> â—ï¸ Warning
>
> Make sure that the command `make data` was executed without errors before trying to run the notebook.

### Data Preprocessing 
The data to download the data also applies the first preprocessing steps. Those functions join both versions of the data and generate the preprocessed dataset used later on.

### Modeling
The first step of the modeling is to  generate the features. This could be done running the command:
```{bash}
make features
```
The process of computing the features is broadly explained in the notebook `002-preprocessing.ipynb`.

Then, to train the models, run:
```{bash}
make models
```
The multiple models tested along with the results can be found in the following notebooks: 

- Topic modeling: `003-topicmodels.ipynb`.
- Regression: `004-regression.ipynb`.
- Classification: `005-classification.ipynb`.


### Evaluation 
The evaluation of the trained models can be found in their respectives notebooks.

### Deploy

![](reports/figures/deploy_img.png)

After training the models (`make models`), the demo script can be executed with the following command:
```{bash}
make deploy
```
That will open a tab in an internet browser. Using Google Chrome is recommended.


## Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           
    â”œâ”€â”€ README.md          
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
